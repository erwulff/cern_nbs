{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569765b6-99a4-404c-80c9-dcb1f54dcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(\"/mnt/ceph/users/ewulff/particleflow/mlpf\")\n",
    "sys.path.append(\"/mnt/ceph/users/ewulff/particleflow/parameters\")\n",
    "from tfmodel.utils import (\n",
    "    get_num_gpus,\n",
    "    get_strategy,\n",
    "    parse_config,\n",
    "    get_best_checkpoint,\n",
    "    model_scope,\n",
    ")\n",
    "from tfmodel.datasets.BaseDatasetFactory import mlpf_dataset_from_config\n",
    "from tfmodel.callbacks import NpEncoder\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21986de3-a52b-404e-8fa8-090ce345d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(train_dir=None, weights=None, bs=1, nevents=1000, verbose=0, num_runs=2, output=None, cpus=None):\n",
    "\n",
    "    strategy, num_gpus, num_batches_multiplier = get_strategy(num_cpus=cpus)  # sets TF ENV variables to use num_cpus\n",
    "    assert num_gpus < 2, \"Multi-GPU inference is not supported\"\n",
    "\n",
    "    if output:\n",
    "        assert num_runs > 1, \"If writing summary results to file, num_runs must be >1\"\n",
    "\n",
    "    if train_dir is None:\n",
    "        assert ((config is not None) and (weights is not None)), \"Please provide a config and weight file when not giving train_dir\"\n",
    "\n",
    "\n",
    "    config = Path(train_dir) / \"config.yaml\"\n",
    "    assert config.exists(), \"Could not find config file in train_dir, please provide one with -c <path/to/config>\"\n",
    "    config, _ = parse_config(config, weights=weights)\n",
    "\n",
    "    # disable small graph optimization for onnx export (tf.cond is not well supported by ONNX export)\n",
    "    if \"small_graph_opt\" in config[\"setup\"]:\n",
    "        config[\"setup\"][\"small_graph_opt\"] = False\n",
    "\n",
    "    if not weights:\n",
    "        weights = get_best_checkpoint(train_dir)\n",
    "        logging.info(\"Loading best weights that could be found from {}\".format(weights))\n",
    "\n",
    "    model, _, initial_epoch = model_scope(config, 1, weights=weights)\n",
    "\n",
    "    print(\"before loading\")\n",
    "    print(\"model.normalizer.mean:\", model.normalizer.mean)\n",
    "    print(\"model.normalizer.variance:\", model.normalizer.variance)\n",
    "\n",
    "    cache = np.load(config[\"setup\"][\"normalizer_cache\"] + \".npz\")\n",
    "    model.normalizer.mean = tf.convert_to_tensor(cache[\"mean\"])\n",
    "    model.normalizer.variance = tf.convert_to_tensor(cache[\"variance\"])\n",
    "    print(\"after loading\")\n",
    "    print(\"model.normalizer.mean:\", model.normalizer.mean)\n",
    "    print(\"model.normalizer.variance:\", model.normalizer.variance)\n",
    "\n",
    "    num_events = nevents if nevents >= 0 else config[\"validation_num_events\"]\n",
    "    ds_val = mlpf_dataset_from_config(\n",
    "        config[\"validation_dataset\"],\n",
    "        config,\n",
    "        \"test\",\n",
    "        num_events,\n",
    "    )\n",
    "    tfds_dataset = ds_val.tensorflow_dataset.padded_batch(bs)\n",
    "\n",
    "    times = []\n",
    "    predict_workers = bs  # 1 worker per sample in the batch\n",
    "    #TODO: don't hardcode maximum allowed workers\n",
    "    if predict_workers > 112:\n",
    "        predict_workers = 112  # ensure workers is not more than available cpu threads\n",
    "    for i in range(num_runs):\n",
    "        # Using model.predict(tf_dataset) doesn't work because pfnetdense is not hashable due to the use of slicing in\n",
    "        # __call__. Hence, we have to loop through the dataset.\n",
    "        print(\"\\nRun {}/{}\".format(i + 1, num_runs))\n",
    "        print(\"####### Inference using model.predict(sample) ############\")\n",
    "        start_time = tf.timestamp().numpy()\n",
    "        for elem in tqdm(tfds_dataset, desc=\"Model inference\"):\n",
    "            ypred = model.predict(\n",
    "                elem[\"X\"],\n",
    "                verbose=verbose,\n",
    "                workers=predict_workers,\n",
    "                use_multiprocessing=(predict_workers > 1),\n",
    "            )\n",
    "            # ypred[\"charge\"] = np.argmax(ypred[\"charge\"], axis=-1) - 1\n",
    "            # ypred[\"cls_id\"] = tf.math.argmax(ypred[\"cls\"], axis=-1).numpy()\n",
    "        stop_time = tf.timestamp().numpy()\n",
    "        total_time = stop_time - start_time\n",
    "        times.append(total_time)\n",
    "        print(\"Total number of events used: {:d}\".format(num_events))\n",
    "        print(\"Batch size: {:d}\".format(bs))\n",
    "        print(\"Total inference time: {:.2f}s\".format(total_time))\n",
    "        print(\"##########################################################\")\n",
    "\n",
    "    if num_runs > 1:\n",
    "        # Summarizing results\n",
    "\n",
    "        # event throughput [1/s]\n",
    "        #   - ignore batch padding\n",
    "        throughput_per_run = num_events / np.array(times)\n",
    "\n",
    "        # mean throughput\n",
    "        #   - ignore first epoch (lazy graph construction)\n",
    "        mean_throughput = round(np.mean(throughput_per_run[1:]), 4)\n",
    "        print(\"mean_throughput:\", mean_throughput)\n",
    "\n",
    "        # mean epoch time\n",
    "        #   - ignore first epoch (lazy graph construction)\n",
    "        mean_run_time = round(np.mean(times[1:]), 4)\n",
    "        # batch_size_total = bs * (num_gpus or num_cpus)\n",
    "        print(\"mean_run_time:\", mean_run_time)\n",
    "\n",
    "        data = {\n",
    "            \"results\": [{\n",
    "                \"wl-scores\": {\n",
    "                    \"mean_throughput\": mean_throughput,\n",
    "                    \"mean_run_time\": mean_run_time,\n",
    "                },\n",
    "                \"wl-stats\": {\n",
    "                    \"num_runs\": len(times),\n",
    "                    \"run_times\": np.round(times, 4),\n",
    "                    \"total_inference_time\": round(sum(times[1:]), 4),\n",
    "                    \"GPU\": num_gpus,\n",
    "                    \"CPU\": cpus or -1,\n",
    "                    # \"train_set_size\": self.train_set_size,\n",
    "                    # \"batch_size_per_device\": self.batch_size_per_gpu,\n",
    "                    # \"batch_size_total\": batch_size_total,\n",
    "                    \"batch_size\": bs,\n",
    "                    \"steps_per_run\": num_events // bs,\n",
    "                    \"events_per_run\": num_events,\n",
    "                    \"throughput_per_run\": list(np.round(throughput_per_run, 4)),\n",
    "                },\n",
    "            }],\n",
    "        }\n",
    "\n",
    "        if output:\n",
    "            result_path = output.resolve()\n",
    "            result_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if result_path.is_file():\n",
    "                with result_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                    old_data = json.load(f)\n",
    "            else:\n",
    "                old_data = None    \n",
    "            \n",
    "            with result_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                if old_data:\n",
    "                    data = {\"results\": old_data[\"results\"] + data[\"results\"]}\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4, cls=NpEncoder)\n",
    "                f.write(\"\\n\")\n",
    "            print(\"Saved result to {}\".format(result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2da1cd-9263-4f04-9f57-fbad31b1a97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = \"/mnt/ceph/users/ewulff/particleflow/experiments/bsm10_1GPU_clic-gnn-tuned-v130_20230724_035617_375578.workergpu037\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3eae5-568a-4ad1-8e90-4cbf101c9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bs in [256, 128, 64, 32, 16, 8, 1]:\n",
    "for bs in [1024, 512, 4, 2]:\n",
    "    infer(\n",
    "        train_dir=train_dir,\n",
    "        bs=bs,\n",
    "        nevents=4000,\n",
    "        num_runs=11,\n",
    "        output=Path(\"/mnt/ceph/users/ewulff/particleflow/inference_tests/ipynb_results_worker6301.json\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8177e56-1e98-4a4b-82a5-1012895be2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
